{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10174490,
          "sourceType": "datasetVersion",
          "datasetId": 6192936
        }
      ],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial Neural Networks and Deep Learning"
      ],
      "metadata": {
        "id": "EzkCqN31zuhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ Import Libraries"
      ],
      "metadata": {
        "id": "KBfNQCdlzuho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Import TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "import keras\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "# Reduce TensorFlow verbosity\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(tf.__version__)\n",
        "\n",
        "# Import other libraries\n",
        "import albumentations as A\n",
        "import os\n",
        "import math\n",
        "from PIL import Image\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from keras.utils import to_categorical\n",
        "from keras.saving import load_model\n",
        "from keras.metrics import MeanIoU\n",
        "from keras import saving as ks\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install -U segmentation-models\n",
        "import segmentation_models as sm\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {tfk.__version__}\")\n",
        "print(f\"GPU devices: {len(tf.config.list_physical_devices('GPU'))}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "6-R5FDLJzuhp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ Functions"
      ],
      "metadata": {
        "id": "LfwpxyNHzuhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data visualization - plot random samples\n",
        "def plot_images(images, masks, num_samples=3):\n",
        "    random_indices = np.random.choice(len(images), num_samples, replace=False)\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 2, figsize=(10, 4 * num_samples))\n",
        "    for i, idx in enumerate(random_indices):\n",
        "        axes[i, 0].imshow(images[idx], cmap='gray')\n",
        "        axes[i, 0].set_title(f\"Image {idx}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        axes[i, 1].imshow(masks[idx], cmap='viridis')\n",
        "        axes[i, 1].set_title(f\"Label {idx}\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "trusted": true,
        "id": "lAVUx4Okzuhq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_images_with_class(images, masks, target_class):\n",
        "    filtered_images = []\n",
        "    filtered_masks = []\n",
        "\n",
        "    for img, mask in zip(images, masks):\n",
        "        if target_class in mask:\n",
        "            filtered_images.append(img)\n",
        "            filtered_masks.append(mask)\n",
        "\n",
        "    return np.array(filtered_images), np.array(filtered_masks)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7w8Yy90Dzuhq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_classwise_miou(y_true, y_pred, num_classes):\n",
        "    # Flatten the arrays\n",
        "    y_true_flat = y_true.flatten()\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "\n",
        "    # Initialize the MeanIoU object\n",
        "    miou_metric = tfk.metrics.MeanIoU(num_classes=num_classes)\n",
        "    miou_metric.update_state(y_true_flat, y_pred_flat)\n",
        "\n",
        "    # Extract IoU per class\n",
        "    total_conf_matrix = miou_metric.total_cm.numpy()  # Get the confusion matrix\n",
        "    ious = []\n",
        "    for i in range(num_classes):\n",
        "        TP = total_conf_matrix[i, i]  # True positives for class i\n",
        "        FP = total_conf_matrix[:, i].sum() - TP  # False positives for class i\n",
        "        FN = total_conf_matrix[i, :].sum() - TP  # False negatives for class i\n",
        "        denominator = TP + FP + FN\n",
        "        if denominator == 0:\n",
        "            iou = np.nan  # Handle classes not present in predictions or labels\n",
        "        else:\n",
        "            iou = TP / denominator\n",
        "        ious.append(iou)\n",
        "\n",
        "    # Create a dictionary for class-wise IoU\n",
        "    miou_per_class = {f\"Class {i}\": iou for i, iou in enumerate(ious)}\n",
        "    return miou_per_class"
      ],
      "metadata": {
        "trusted": true,
        "id": "eHRY6iGyzuhr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_augmentations(images, masks, aug_list):\n",
        "    augmented_images = []\n",
        "    augmented_masks = []\n",
        "\n",
        "    for aug in aug_list:\n",
        "        for img, mask in zip(images, masks):\n",
        "            augmented = aug(image=img, mask=mask)\n",
        "            augmented_images.append(augmented['image'])\n",
        "            augmented_masks.append(augmented['mask'])\n",
        "\n",
        "    augmented_images = np.array(augmented_images)\n",
        "    augmented_masks = np.array(augmented_masks)\n",
        "\n",
        "    return {\"images\":augmented_images, \"labels\":augmented_masks}"
      ],
      "metadata": {
        "trusted": true,
        "id": "wGS7Sazjzuhr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_distribution(masks, num_classes):\n",
        "    # Flatten all masks into a 1D array to count pixel occurrences\n",
        "    flat_masks = masks.flatten()\n",
        "\n",
        "    # Count the occurrences of each class label\n",
        "    class_counts = Counter(flat_masks)\n",
        "\n",
        "    # Calculate total number of pixels\n",
        "    total_pixels = flat_masks.size\n",
        "\n",
        "    # Calculate percentage distribution\n",
        "    percentages = {label: (count / total_pixels) * 100 for label, count in class_counts.items()}\n",
        "\n",
        "    return class_counts, percentages"
      ],
      "metadata": {
        "trusted": true,
        "id": "VSr3yjafzuhr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_weights_from_dict(class_percentage_dict):\n",
        "    # Exclude class 0 from the computation\n",
        "    class_labels = list(class_percentage_dict.keys())\n",
        "    class_percentages = np.array(list(class_percentage_dict.values()))\n",
        "\n",
        "    # Identify indices for all classes except 0\n",
        "    non_zero_indices = [i for i, label in enumerate(class_labels) if label != 0]\n",
        "\n",
        "    # Get class labels and percentages for non-zero classes\n",
        "    non_zero_labels = [class_labels[i] for i in non_zero_indices]\n",
        "    non_zero_percentages = class_percentages[non_zero_indices]\n",
        "\n",
        "    # Normalize percentages to sum to 1 (excluding class 0)\n",
        "    class_frequencies = non_zero_percentages / np.sum(non_zero_percentages)\n",
        "\n",
        "    # Compute inverse frequencies and scale to average 1\n",
        "    inverse_frequencies = 1.0 / (class_frequencies + 1e-6)\n",
        "    scaled_weights = inverse_frequencies / np.mean(inverse_frequencies)\n",
        "\n",
        "    # Create a dictionary mapping non-zero labels to weights\n",
        "    class_weights = dict(zip(non_zero_labels, scaled_weights))\n",
        "\n",
        "    # Add class 0 with a weight of 0\n",
        "    class_weights[0.0] = 0.0\n",
        "\n",
        "    # Ensure all classes are present in the output dictionary\n",
        "    for label in class_labels:\n",
        "        if label not in class_weights:\n",
        "            class_weights[label] = 0.0\n",
        "\n",
        "    return class_weights"
      ],
      "metadata": {
        "trusted": true,
        "id": "azJpSt5Czuhs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sample_weights(labels, class_weights):\n",
        "    # Convert the class_weights dictionary to a weight vector\n",
        "    weight_vector = np.array([class_weights[class_id] for class_id in range(len(class_weights))])\n",
        "\n",
        "    # Multiply one-hot labels with the weight vector\n",
        "    weighted_labels = labels * weight_vector[np.newaxis, np.newaxis, np.newaxis, :]  # Broadcast weights\n",
        "\n",
        "    # Sum over the class channel to get per-pixel weights\n",
        "    sample_weights = np.sum(weighted_labels, axis=-1)  # Shape: (batch_size, height, width)\n",
        "\n",
        "    return sample_weights"
      ],
      "metadata": {
        "trusted": true,
        "id": "TOv-h-Ydzuhs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    # Plot and display training and validation loss\n",
        "    plt.figure(figsize=(18, 3))\n",
        "    plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n",
        "    plt.plot(history['val_loss'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n",
        "    plt.title('Cross Entropy')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot and display training and validation accuracy\n",
        "    plt.figure(figsize=(18, 3))\n",
        "    plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n",
        "    plt.plot(history['val_accuracy'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot and display training and validation mean IoU\n",
        "    plt.figure(figsize=(18, 3))\n",
        "    plt.plot(history['meaniou'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n",
        "    plt.plot(history['val_meaniou'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n",
        "    plt.title('Mean Intersection over Union')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "2opaTxwXzuhs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⏳ Load and Process the Data"
      ],
      "metadata": {
        "id": "PA6Sl1i1zuht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(\"/kaggle/input/siumgpt-homework2-datasets/mars_for_students.npz\")\n",
        "\n",
        "training_set = data[\"training_set\"]\n",
        "X_train = training_set[:, 0]\n",
        "y_train = training_set[:, 1]\n",
        "\n",
        "X_test = data[\"test_set\"]\n",
        "\n",
        "labels = {\n",
        "    0: \"Background\",\n",
        "    1: \"Soil\",\n",
        "    2: \"Bedrock\",\n",
        "    3: \"Sand\",\n",
        "    4: \"Big Rock\"\n",
        "}\n",
        "\n",
        "print(f\"Training X shape: {X_train.shape}\")\n",
        "print(f\"Training y shape: {y_train.shape}\")\n",
        "print(f\"Test X shape: {X_test.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "LdU2KhAnzuht"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing aliens\n",
        "alien_label = y_train[1834, :]\n",
        "plt.imshow(alien_label)\n",
        "\n",
        "filter = [True] * X_train.shape[0]\n",
        "removed = 0\n",
        "print(len(filter))\n",
        "for i, lab in enumerate(y_train):\n",
        "    if np.array_equal(alien_label, lab):\n",
        "        filter[i] = False\n",
        "        removed += 1\n",
        "\n",
        "X_train = X_train[filter]\n",
        "y_train = y_train[filter]\n",
        "\n",
        "print(f\"Training X shape: {X_train.shape}\")\n",
        "print(f\"Training y shape: {y_train.shape}\")\n",
        "print(f'removed {removed} images')"
      ],
      "metadata": {
        "trusted": true,
        "id": "qfKgptYJzuht"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Add color channel\n",
        "X_train = X_train[..., np.newaxis]/255\n",
        "X_test = X_test[..., np.newaxis]/255\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "print(f\"Input shape: {input_shape}\")\n",
        "print(f\"Number of classes: {num_classes}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "vJ22YAQszuht"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "h_flip = A.HorizontalFlip(p=1.0)\n",
        "v_flip = A.VerticalFlip(p=1.0)\n",
        "rotation = A.Affine(rotate=180,p=1)\n",
        "\n",
        "augmentations = [h_flip, v_flip, rotation]\n",
        "aug_dataset = apply_augmentations(X_train, y_train, augmentations)\n",
        "X_train, y_train = aug_dataset['images'], aug_dataset['labels']\n",
        "\n",
        "print(f\"Training X shape: {X_train.shape}\")\n",
        "print(f\"Training y shape: {y_train.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "y0SSv6m3zuht"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts, class_percentages = compute_class_distribution(y_train, num_classes)\n",
        "class_weights = compute_class_weights_from_dict(class_percentages)\n",
        "\n",
        "# Print results\n",
        "print(\"Class Counts:\", list(class_counts.values()))\n",
        "print(\"Class Percentages:\", list(class_percentages.values()))\n",
        "print(\"Computed Class Weights:\", class_weights)\n",
        "\n",
        "# Visualize class distribution\n",
        "plt.bar(class_percentages.keys(), class_percentages.values())\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Percentage')\n",
        "plt.title('Class Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "rkuBoZ7vzuht"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting in train-validation sets\n",
        "train_img, val_img, train_lbl, val_lbl = train_test_split(\n",
        "    X_train, y_train, test_size=0.1, random_state=seed\n",
        ")\n",
        "print(\"Data splitted!\")\n",
        "\n",
        "train_lbl_cat = tf.one_hot(train_lbl, depth=num_classes)\n",
        "val_lbl_cat = tf.one_hot(val_lbl, depth=num_classes)\n",
        "\n",
        "print(f\"\\nNumber of images:\")\n",
        "print(f\"Train: {train_img.shape}\")\n",
        "print(f\"Validation: {val_img.shape}\")\n",
        "print(f\"\\nLabels shape:\")\n",
        "print(f\"Train: {train_lbl_cat.shape}\")\n",
        "print(f\"Validation: {val_lbl_cat.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "H4YagsK3zuht"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🛠️ Train and Save the Model"
      ],
      "metadata": {
        "id": "wqIeBx1-zuht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unet_block(input_tensor, filters, kernel_size=3, activation='relu', stack=2, name=''):\n",
        "    # Initialise the input tensor\n",
        "    x = input_tensor\n",
        "\n",
        "    # Apply a sequence of Conv2D, Batch Normalisation, and Activation layers for the specified number of stacks\n",
        "    for i in range(stack):\n",
        "        x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', name=name + 'conv' + str(i + 1))(x)\n",
        "        x = tfkl.BatchNormalization(name=name + 'bn' + str(i + 1))(x)\n",
        "        x = tfkl.Activation(activation, name=name + 'activation' + str(i + 1))(x)\n",
        "\n",
        "    # Return the transformed tensor\n",
        "    return x"
      ],
      "metadata": {
        "trusted": true,
        "id": "c3ivSXyBzuhu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ks.get_custom_objects().clear()\n",
        "\n",
        "@ks.register_keras_serializable()\n",
        "class AdaptiveFusion(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_inputs, **kwargs):\n",
        "        super(AdaptiveFusion, self).__init__(**kwargs)\n",
        "        self.num_inputs = num_inputs\n",
        "        self.gates = []\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Creazione dei gate trainabili\n",
        "        self.gates = [\n",
        "            self.add_weight(\n",
        "                shape=(1,),\n",
        "                initializer=\"ones\",\n",
        "                trainable=True,\n",
        "                name=f\"gate_{i}\"\n",
        "            )\n",
        "            for i in range(self.num_inputs)\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Verifica che il numero di ingressi corrisponda al numero di gate\n",
        "        if len(inputs) != self.num_inputs:\n",
        "            raise ValueError(f\"Expected {self.num_inputs} inputs, but got {len(inputs)}.\")\n",
        "\n",
        "        # Calcolo della somma pesata\n",
        "        weighted_inputs = [gate * inp for gate, inp in zip(self.gates, inputs)]\n",
        "        output = tf.add_n(weighted_inputs)  # Somma i tensori ponderati\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_inputs\": self.num_inputs})\n",
        "        return config"
      ],
      "metadata": {
        "trusted": true,
        "id": "0iLLNNCYzuhu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def adaptive_multi_supervision(inputs, filters=1, kernel_size=3, activation='relu'):\n",
        "    conv_inputs = []\n",
        "    for i, inp in enumerate(inputs):\n",
        "        x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', name=f'ams_conv_{i}')(inp)\n",
        "        x = tfkl.BatchNormalization(name=f'ams_bn_{i}')(x)\n",
        "        x = tfkl.Activation(activation, name=f'ams_activation_{i}')(x)\n",
        "\n",
        "        upscale_factor = int(64 / inp.shape[1])\n",
        "        if upscale_factor != 1:\n",
        "            x = tfkl.UpSampling2D(size=upscale_factor, interpolation='bilinear')(x)\n",
        "        conv_inputs.append(x)\n",
        "\n",
        "    x =  AdaptiveFusion(num_inputs=len(inputs))(inputs=conv_inputs)\n",
        "    return x"
      ],
      "metadata": {
        "trusted": true,
        "id": "YFqxexCSzuhu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unet_model(input_shape=(64, 128, 1), num_classes=num_classes, seed=seed):\n",
        "    tf.random.set_seed(seed)\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
        "\n",
        "    # Downsampling path 1\n",
        "    down_block_11 = unet_block(input_layer, 64, name='down_block11_')\n",
        "    d11 = tfkl.MaxPooling2D()(down_block_11)\n",
        "\n",
        "    down_block_12 = unet_block(d11, 128, name='down_block12_')\n",
        "    d12 = tfkl.MaxPooling2D()(down_block_12)\n",
        "\n",
        "    down_block_13 = unet_block(d12, 256, name='down_block13_')\n",
        "    d13 = tfkl.MaxPooling2D()(down_block_13)\n",
        "\n",
        "    # Bottleneck\n",
        "    bottleneck_1 = unet_block(d13, 512, name='bottleneck1')\n",
        "\n",
        "    # Upsampling path\n",
        "    u13 = tfkl.UpSampling2D()(bottleneck_1)\n",
        "    u13 = tfkl.Concatenate()([u13,down_block_13])\n",
        "    up_block_13 = unet_block(u13, 256, name='up_block13_')\n",
        "\n",
        "    u12 = tfkl.UpSampling2D()(up_block_13)\n",
        "    u12 = tfkl.Concatenate()([u12,down_block_12])\n",
        "    up_block_12 = unet_block(u12, 128, name='up_block12_')\n",
        "\n",
        "    u11 = tfkl.UpSampling2D()(up_block_12)\n",
        "    u11 = tfkl.Concatenate()([u11,down_block_11])\n",
        "    up_block_11 = unet_block(u11, 64, name='up_block11_')\n",
        "\n",
        "    # Downsampling path\n",
        "    d21 = tfkl.Concatenate()([up_block_11,down_block_11])\n",
        "    down_block_21 = unet_block(d21, 64, name='down_block21_')\n",
        "    d21 = tfkl.MaxPooling2D()(down_block_21)\n",
        "\n",
        "    d22 = tfkl.Concatenate()([up_block_12,down_block_12,d21])\n",
        "    down_block_22 = unet_block(d22, 128, name='down_block22_')\n",
        "    d22 = tfkl.MaxPooling2D()(down_block_22)\n",
        "\n",
        "    d23 = tfkl.Concatenate()([up_block_13,down_block_13,d22])\n",
        "    down_block_23 = unet_block(d23, 256, name='down_block23_')\n",
        "    d23 = tfkl.MaxPooling2D()(down_block_23)\n",
        "\n",
        "    # Bottleneck\n",
        "    bottleneck_2 = tfkl.Concatenate()([bottleneck_1,d23])\n",
        "    bottleneck_2 = unet_block(bottleneck_2, 512, name='bottleneck2')\n",
        "\n",
        "    # Upsampling path\n",
        "    u23 = tfkl.UpSampling2D()(bottleneck_2)\n",
        "    u23 = tfkl.Concatenate()([u23,down_block_23,down_block_13,u13])\n",
        "    up_block_23 = unet_block(u23, 256, name='up_block23_')\n",
        "\n",
        "    u22 = tfkl.UpSampling2D()(up_block_23)\n",
        "    u22 = tfkl.Concatenate()([u22,down_block_22,down_block_12,u12])\n",
        "    up_block_22 = unet_block(u22, 128, name='up_block22_')\n",
        "\n",
        "    u21 = tfkl.UpSampling2D()(up_block_22)\n",
        "    u21 = tfkl.Concatenate()([u21,down_block_21,down_block_11,u11])\n",
        "    up_block_21 = unet_block(u21, 64, name='up_block21_')\n",
        "\n",
        "    # Adaptive Multi-Supervision\n",
        "    ams_output = adaptive_multi_supervision(inputs=[up_block_11,up_block_23,up_block_22,up_block_21])\n",
        "\n",
        "    # Output Layer\n",
        "    output_layer = tfkl.Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\", name='output_layer')(ams_output)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='WNet')\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "id": "SK0qmPBbzuhu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_unet_model()\n",
        "model.summary(expand_nested=True, show_trainable=True)\n",
        "tf.keras.utils.plot_model(model, show_trainable=True, expand_nested=True, dpi=70)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pIg0HN8rzuhv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "@ks.register_keras_serializable()\n",
        "class CustomLoss(tfk.losses.Loss):\n",
        "    def __init__(self, k_dice, k_focal, **kwargs):\n",
        "        super(CustomLoss, self).__init__()\n",
        "        self.k_dice = k_dice\n",
        "        self.k_focal = k_focal\n",
        "\n",
        "        self.dice_loss = sm.losses.DiceLoss(class_indexes=[1,2,3,4])\n",
        "        self.focal_loss = sm.losses.CategoricalFocalLoss(class_indexes=[1,2,3,4])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        # Update the config with the custom layer's parameters\n",
        "        config.update(\n",
        "            {\n",
        "                \"k_dice\": self.k_dice,\n",
        "                \"k_focal\": self.k_focal,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        focal = self.focal_loss(y_true, y_pred)\n",
        "\n",
        "        dice = self.dice_loss(y_true, y_pred)\n",
        "\n",
        "        return self.k_dice * dice + self.k_focal * focal"
      ],
      "metadata": {
        "trusted": true,
        "id": "UDeRxdB6zuhv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Reused parameters for all models\n",
        "learning_rate = 1e-3\n",
        "patience = 30\n",
        "batch_size = 64\n",
        "epochs = 1000\n",
        "\n",
        "optimizer = tf.keras.optimizers.AdamW(learning_rate, clipnorm=1.0)\n",
        "mean_iou = tfk.metrics.MeanIoU(num_classes=num_classes, ignore_class=0, sparse_y_true=False, sparse_y_pred=False, name=\"meaniou\")\n",
        "metrics = [\"accuracy\", mean_iou]\n",
        "\n",
        "# Loss function for model\n",
        "loss = CustomLoss(1, 1)\n",
        "\n",
        "sample_weights = compute_sample_weights(train_lbl_cat, class_weights)\n",
        "\n",
        "# Setup callbacks\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_meaniou',\n",
        "    mode='max',\n",
        "    patience=patience,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_meaniou\",\n",
        "    mode='max',\n",
        "    factor=0.1,\n",
        "    patience=20\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "1RL5VniUzuhv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model.compile(\n",
        "    loss=loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_img,\n",
        "    train_lbl_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    sample_weight=sample_weights,\n",
        "    validation_data=(val_img, val_lbl_cat),\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ").history\n",
        "\n",
        "# Calculate and print the final validation accuracy\n",
        "final_val_meanIoU = round(max(history['val_meaniou'])* 100, 2)\n",
        "print(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}%')\n",
        "\n",
        "timestep_str = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
        "model_filename = f\"model_{timestep_str}.keras\"\n",
        "model.save(model_filename)\n",
        "\n",
        "print(f\"Model saved to {model_filename}\")\n",
        "\n",
        "del model"
      ],
      "metadata": {
        "trusted": true,
        "id": "TlqNBYrGzuhv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(history)"
      ],
      "metadata": {
        "trusted": true,
        "id": "x-UyaBFuzuhv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize predictions"
      ],
      "metadata": {
        "id": "YJRs2nlTzuhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If model_filename is not defined, load the most recent model from Google Drive\n",
        "if \"model_filename\" not in globals() or model_filename is None:\n",
        "    files = [f for f in os.listdir('.') if os.path.isfile(f) and f.startswith('model_') and f.endswith('.keras')]\n",
        "    files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
        "    if files:\n",
        "        model_filename = files[0]\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No model files found in the current directory.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ak4FcCEJzuhw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = tfk.models.load_model(model_filename)\n",
        "print(f\"Model loaded from {model_filename}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "rCZRcjdczuhw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "val_lbl_pred = model.predict(val_img)"
      ],
      "metadata": {
        "trusted": true,
        "id": "o-HBYiJ8zuhw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_argmax = np.argmax(val_lbl_pred, axis=-1)  # Convert one-hot predictions to class indices\n",
        "val_lbl_argmax = np.argmax(val_lbl_cat, axis=-1)  # Convert one-hot labels to class indices\n",
        "\n",
        "miou_per_class = calculate_classwise_miou(val_lbl_argmax, y_pred_argmax, num_classes)\n",
        "\n",
        "# Print the IoU scores for each class\n",
        "print(\"Class-wise IoU scores:\")\n",
        "for cls, score in miou_per_class.items():\n",
        "    print(f\"{cls}: {score:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "7AJ2D64Dzuhw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Define the values and corresponding colors\n",
        "values = [0, 1, 2, 3, 4]\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "labels = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
        "\n",
        "# Create colormap and normalization\n",
        "cmap = mcolors.ListedColormap(colors)\n",
        "norm = mcolors.BoundaryNorm(values + [5], cmap.N)  # Add a boundary for the last value\n",
        "\n",
        "# Create legend handles\n",
        "legend_handles = [mpatches.Patch(color=color, label=label) for color, label in zip(colors, labels)]\n",
        "\n",
        "for i in range(100):\n",
        "    # Create a new figure\n",
        "    plt.figure(figsize=(16, 6))\n",
        "\n",
        "    # Add the legend at the top\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.axis('off')  # Turn off the axes\n",
        "    plt.legend(handles=legend_handles, loc='center', ncol=len(values), fontsize=10, frameon=False)\n",
        "    plt.title(\"Legend\", fontsize=12)\n",
        "\n",
        "    # Plot the images\n",
        "    # Input image\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.imshow(val_img[i], cmap='gray')\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Ground truth\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.imshow(val_lbl[i], cmap=cmap, norm=norm)\n",
        "    plt.title(\"Ground Truth Mask\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Prediction\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.imshow(np.argmax(val_lbl_pred[i], axis=-1), cmap=cmap, norm=norm)\n",
        "    plt.title(\"Predicted Mask\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Vt3zOW84zuhz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Prepare Your Submission"
      ],
      "metadata": {
        "id": "JLEsk_LLzuhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If model_filename is not defined, load the most recent model from Google Drive\n",
        "if \"model_filename\" not in globals() or model_filename is None:\n",
        "    files = [f for f in os.listdir('.') if os.path.isfile(f) and f.startswith('model_') and f.endswith('.keras')]\n",
        "    files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
        "    if files:\n",
        "        model_filename = files[0]\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No model files found in the current directory.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "_AVQ0X9Ozuhz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(X_test)\n",
        "preds = np.argmax(preds, axis=-1)\n",
        "print(f\"Predictions shape: {preds.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "YOYQ7a11zuh0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def y_to_df(y) -> pd.DataFrame:\n",
        "    \"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n",
        "    n_samples = len(y)\n",
        "    y_flat = y.reshape(n_samples, -1)\n",
        "    df = pd.DataFrame(y_flat)\n",
        "    df[\"id\"] = np.arange(n_samples)\n",
        "    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n",
        "    return df[cols]"
      ],
      "metadata": {
        "trusted": true,
        "id": "Xd0sv2Xnzuh0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and download the csv submission file\n",
        "timestep_str = model_filename.replace(\"model_\", \"\").replace(\".keras\", \"\")\n",
        "submission_filename = f\"submission_{timestep_str}.csv\"\n",
        "submission_df = y_to_df(preds)\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "\n",
        "%cd /kaggle/working\n",
        "from IPython.display import FileLink\n",
        "FileLink(submission_filename)"
      ],
      "metadata": {
        "trusted": true,
        "id": "UfchGCDXzuh0"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}